{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/anvil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(400)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "document_num = 50\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 18 (\"rest\") appears 1 time.\n",
      "Word 166 (\"clear\") appears 1 time.\n",
      "Word 336 (\"refer\") appears 1 time.\n",
      "Word 350 (\"true\") appears 1 time.\n",
      "Word 391 (\"technolog\") appears 1 time.\n",
      "Word 437 (\"christian\") appears 1 time.\n",
      "Word 453 (\"exampl\") appears 1 time.\n",
      "Word 476 (\"jew\") appears 1 time.\n",
      "Word 480 (\"lead\") appears 1 time.\n",
      "Word 482 (\"littl\") appears 3 time.\n",
      "Word 520 (\"wors\") appears 2 time.\n",
      "Word 721 (\"keith\") appears 3 time.\n",
      "Word 732 (\"punish\") appears 1 time.\n",
      "Word 803 (\"california\") appears 1 time.\n",
      "Word 859 (\"institut\") appears 1 time.\n",
      "Word 917 (\"similar\") appears 1 time.\n",
      "Word 990 (\"allan\") appears 1 time.\n",
      "Word 991 (\"anti\") appears 1 time.\n",
      "Word 992 (\"arriv\") appears 1 time.\n",
      "Word 993 (\"austria\") appears 1 time.\n",
      "Word 994 (\"caltech\") appears 2 time.\n",
      "Word 995 (\"distinguish\") appears 1 time.\n",
      "Word 996 (\"german\") appears 1 time.\n",
      "Word 997 (\"germani\") appears 3 time.\n",
      "Word 998 (\"hitler\") appears 1 time.\n",
      "Word 999 (\"livesey\") appears 2 time.\n",
      "Word 1000 (\"motto\") appears 2 time.\n",
      "Word 1001 (\"order\") appears 1 time.\n",
      "Word 1002 (\"pasadena\") appears 1 time.\n",
      "Word 1003 (\"pompous\") appears 1 time.\n",
      "Word 1004 (\"popul\") appears 1 time.\n",
      "Word 1005 (\"rank\") appears 1 time.\n",
      "Word 1006 (\"schneider\") appears 1 time.\n",
      "Word 1007 (\"semit\") appears 1 time.\n",
      "Word 1008 (\"social\") appears 1 time.\n",
      "Word 1009 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "      \n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"presid\" + 0.004*\"clinton\" + 0.004*\"bike\" + 0.003*\"netcom\" + 0.003*\"run\" + 0.003*\"homosexu\" + 0.003*\"talk\" + 0.003*\"pitch\" + 0.003*\"money\" + 0.003*\"virginia\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.009*\"govern\" + 0.007*\"armenian\" + 0.006*\"israel\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.004*\"american\" + 0.004*\"turkish\" + 0.004*\"weapon\" + 0.004*\"jew\" + 0.004*\"countri\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.017*\"game\" + 0.015*\"team\" + 0.011*\"play\" + 0.009*\"player\" + 0.008*\"hockey\" + 0.006*\"season\" + 0.005*\"canada\" + 0.005*\"leagu\" + 0.005*\"score\" + 0.004*\"divis\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"card\" + 0.009*\"window\" + 0.007*\"driver\" + 0.007*\"sale\" + 0.006*\"price\" + 0.005*\"speed\" + 0.005*\"appl\" + 0.005*\"video\" + 0.005*\"monitor\" + 0.004*\"engin\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.015*\"file\" + 0.010*\"program\" + 0.009*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\" + 0.006*\"imag\" + 0.006*\"data\" + 0.006*\"avail\" + 0.005*\"version\" + 0.005*\"code\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.013*\"space\" + 0.010*\"nasa\" + 0.006*\"scienc\" + 0.005*\"research\" + 0.005*\"orbit\" + 0.004*\"launch\" + 0.003*\"pitt\" + 0.003*\"earth\" + 0.003*\"develop\" + 0.003*\"center\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.035*\"drive\" + 0.015*\"scsi\" + 0.011*\"disk\" + 0.009*\"hard\" + 0.009*\"control\" + 0.007*\"columbia\" + 0.006*\"washington\" + 0.005*\"car\" + 0.005*\"uiuc\" + 0.004*\"floppi\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"word\" + 0.005*\"religion\" + 0.004*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: help\n",
      "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
      "Lines: 13\n",
      "\n",
      "Hello All!\n",
      "\n",
      "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
      "prior to starting Windows - this makes getting into Windows quite slow if you\n",
      "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
      "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
      "maybe make something like a PIF file to load them only when you enter the\n",
      "applications that need fonts?  Any ideas?\n",
      "\n",
      "\n",
      "Chris\n",
      "\n",
      " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5578656196594238\t Topic: 0.010*\"card\" + 0.009*\"window\" + 0.007*\"driver\" + 0.007*\"sale\" + 0.006*\"price\"\n",
      "Score: 0.41789543628692627\t Topic: 0.015*\"file\" + 0.010*\"program\" + 0.009*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\"\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  5  0 ...  9  6 15]\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.read_json('./data/youtube_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2810 entries, 0 to 2809\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   url              2810 non-null   object\n",
      " 1   title            2810 non-null   object\n",
      " 2   visit_count      2810 non-null   int64 \n",
      " 3   last_visit_time  2810 non-null   int64 \n",
      " 4   publishedAt      2810 non-null   object\n",
      " 5   description      2810 non-null   object\n",
      " 6   channelTitle     2810 non-null   object\n",
      " 7   channelId        2810 non-null   object\n",
      " 8   tags             2162 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 197.7+ KB\n",
      "None\n",
      "                                                url  \\\n",
      "0       https://www.youtube.com/watch?v=kw9nTK42bTw   \n",
      "1       https://www.youtube.com/watch?v=D9veJLKqnpg   \n",
      "2       https://www.youtube.com/watch?v=ESKDxvPeUUE   \n",
      "3       https://www.youtube.com/watch?v=RxEA1WleXlM   \n",
      "4       https://www.youtube.com/watch?v=PzyvTHrLmQk   \n",
      "5       https://www.youtube.com/watch?v=lURxO6NaEcY   \n",
      "6  https://www.youtube.com/watch?v=lURxO6NaEcY&t=0s   \n",
      "7       https://www.youtube.com/watch?v=nvOFbnQwJsc   \n",
      "8       https://www.youtube.com/watch?v=KA3uHtGUj0s   \n",
      "9       https://www.youtube.com/watch?v=YY0VaSjPV1Y   \n",
      "\n",
      "                                               title  visit_count  \\\n",
      "0  Build ENTIRE Apps With A Single Prompt - FREE ...            1   \n",
      "1  Let's build a room sensor - Part 1 - Temperatu...            1   \n",
      "2  Everyone's Racing To Replace Redis - Who Will ...            2   \n",
      "3                  Anthropic Claude Prompt Generator            1   \n",
      "4  Huginn: Free Open Source Automated Agents Plat...            1   \n",
      "5  Learn AI Engineer Skills For Beginners: OpenAI...            1   \n",
      "6  Learn AI Engineer Skills For Beginners: OpenAI...            1   \n",
      "7  How To Export Bookmarks From Brave Browser - T...            1   \n",
      "8  Forever Alone - A New Russian Tank Tactic? - (...            2   \n",
      "9  LlamaIndex Webinar: Building an LLM-powered Br...            1   \n",
      "\n",
      "     last_visit_time           publishedAt  \\\n",
      "0  13356893022266968  2024-04-03T16:41:35Z   \n",
      "1  13356893020680892  2024-03-08T14:01:42Z   \n",
      "2  13356893018097883  2024-04-04T07:47:52Z   \n",
      "3  13356893018041099  2024-04-02T21:30:47Z   \n",
      "4  13356893017575846  2024-04-05T08:30:07Z   \n",
      "5  13356893013028224  2023-10-27T14:29:31Z   \n",
      "6  13356893011221479  2023-10-27T14:29:31Z   \n",
      "7  13356892978107304  2023-09-17T20:42:31Z   \n",
      "8  13356841372749473  2024-04-04T11:14:01Z   \n",
      "9  13356840947523644  2024-04-02T20:44:03Z   \n",
      "\n",
      "                                         description        channelTitle  \\\n",
      "0  Devika is the most popular open-source Devin c...      Matthew Berman   \n",
      "1  I am aware of some glitches in the video, I wa...      Home Automator   \n",
      "2  Valkey, KeyDB, and more are quickly gaining tr...        Theo - t3․gg   \n",
      "3  Master Prompt Engineering with Anthropic's Hel...        Kyle Behrend   \n",
      "4  Discover Huginn, a free open-source platform f...             Elestio   \n",
      "5  Learn AI Engineer Skills For Beginners: OpenAI...        All About AI   \n",
      "6  Learn AI Engineer Skills For Beginners: OpenAI...        All About AI   \n",
      "7  Your carefully curated bookmark collection in ...  Tutorial Workspace   \n",
      "8  Uncensored Video at:\\nhttps://open.substack.co...         Ryan McBeth   \n",
      "9  Learn how to build an AI Browser Copilot 🤖🌐\\n\\...          LlamaIndex   \n",
      "\n",
      "                  channelId                                               tags  \n",
      "0  UCawZsQWqfGSbCI5yjkdVkTA  [devika, ai coding, open ai, llm, ai, open sou...  \n",
      "1  UCeiWwT0ZweB4Oy_vAOPZ3lQ  [ESPHome, DHT22, ESP, ESP32, Home Assistant, B...  \n",
      "2  UCbRP3c757lWg9M-U7TyEkXA  [web development, full stack, typescript, java...  \n",
      "3  UCdslap03fm-2Mj4mO-QBLuA                                                NaN  \n",
      "4  UC-hHhwIlSK68yyw7ZuH8Q4g  [open source software, open source alternative...  \n",
      "5  UCR9j1jqqB5Rse69wjUnbYwA  [ai, chatgpt, ai engineer, ai engineering, ai ...  \n",
      "6  UCR9j1jqqB5Rse69wjUnbYwA  [ai, chatgpt, ai engineer, ai engineering, ai ...  \n",
      "7  UCiHT0h7RNDQjHqu4j5Gy-ew  [how to export bookmarks in brave, how to expo...  \n",
      "8  UC8URMa1fI4rlaLc-Lhev2fQ                                                NaN  \n",
      "9  UCeRjipR4_SsCddq9VZ2AeKg                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "print(videos.info())\n",
    "processed_docs = []\n",
    "\n",
    "print(videos.head(10))\n",
    "# for doc in videos.itertuples():\n",
    "# \t\tprint(doc)\n",
    "    #processed_docs.append(preprocess(doc['soup']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
